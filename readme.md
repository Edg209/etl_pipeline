# Readme

This readme will detail the pipeline setup in this project.
It will explain the concepts involved, the reason for the design choices, and how future similar developments can be implemented.

## Purpose of this project

This project is a python based ETL pipeline framework, with an example pipeline created with that framework. The example pipeline connects to the [Intercom API](https://developers.intercom.com/intercom-api-reference/reference/welcome) and loads the [conversation model](https://developers.intercom.com/intercom-api-reference/reference/the-conversation-model) to a postgres database.

## Running locally

In order to run the pipeline locally, the JSON `credentials.json` must be populated.
In a non-local environment there would be environment variables giving environment information or sensitive credentials, and when running locally we expect these to be contained in `credentials.json`.

The file `run_local.py` can be used to run the pipeline.
It requires `credentials.json` to be populated first.
It will set the environment variables, recreate all database tables, and then run the conversation pipeline.

There is some database setup that is not performed by the `run_local script`. We need the following:

* A database must exist with the details matching `credentials.json`
* The database must have a user with username and password matching `credentials.json`
* The database must have a schema named `intercom`
* The database user detailed in `credentials.json` must have permissions to drop and create tables on the schema `intercom`

When this is set up, an example of the logs generated can be found in `example_log_full.log` and `example_log_empty.log`.
These files contain the logs generated by a full historical load and a pipeline run with no changes respectively.

## The database

### Database objects

The structure of the database objects is contained within the folder `database_objects`.

The database objects follow the naming convention `database_objects\[OBJECT_SCHEMA]\[OBJECT_TYPE]\[OBJECT_NAME].sql`.

The current version of the code only has a single schema expected - `intercom` for data from the intercom api. Future developments would add new schemas and new objects to the existing schemas.

Proper deployment scripts are outside the scope of this project, instead `run_local` does a simple drop and recreate of all the objects.

### Interation with the database

The methods to interact with the database are contained within the module `database`.
The module `psycopg` is used to connect to the database and execute queries.

## Pipeline logic

The generic pipeline logic is contained in the module `pipeline`.

### Pipeline concepts

A pipeline is a method to process payloads by applying logic steps to that pipeline. A single pipeline run can process any number of payloads.

A payload represents an atomic unit of data to process.
Each payload is represented by a `payload` object what will be updated as the payload is processed.
There is flexibility of how finely the work should be divided between payloads - the only strong guideline is that a single payload should be small enough that it can be processed without encountering memory issues.
The payloads should be able to be run independently of each other. In this current version of the code the payloads are run in sequence, but a likely future development would be to run them in parallel.

The steps of logic that are contained within a pipeline are divided into multiple categories, each of which has a base class that must be inherited from.
Each of these base classes has a method named `execute` that is expected to contain the logic for that step.
The different logical steps are:

* `PreExecutionStep` - Before any of the payloads are processed, these are run. They are run in the order given by the pipeline attribute `pre_execution_steps`. These are expected to run exactly once each, independent of the number of payloads, and they will modify the attributes of the `Pipeline` object.
* `PipelineStep` - As part of processing a payload, these are run. They are run in the order given by the pipeline attribute `pipeline_steps`. They are run once per payload, and all steps will be run for one payload before moving onto the next payload.
* `PostExecutionStep` - These are run after all the payloads are processed. These are expected to run exactly once each, independent of the number of payloads, and they will modify the attributes of the `Pipeline` object. They will be run in the order given by the pipeline attribute `post_execution_steps`.
* `ErrorHandlingStep` - In normal pipeline execution, these will not be run. They will only be run if there is an exception raised while executing a pipeline. They will be run in the order given by the pipeline attribute `error_handling_steps`.

The payloads to be run are contained in the attribute `payloads`.
As the pipeline is executed, this list will be depopulated.
The payloads can be directly set before execution, or they can be set by a `PreExecutionStep`.

### Pipeline exceptions

There are two special exceptions that can be raised by the pipeline steps: `StopPayloadException` and `StopPipelineException`.
These are expected to be raised when we decide that we do not want to continue execution, but do not want to consider the pipeline errored - for example, if we determine that there are no data changes between pipeline runs then one of these might be raised.

A `StopPayloadException` will stop the processing of the current payload, and the next payload will be processed (if there is one).
A `StopPipelineException` will stop the current payload and prevent subsequent payloads from being processed.
The post-execution steps will still be run for both of these.

If an exception is raised other than `StopPayloadException` and `StopPipelineException`, then the pipeline is considered failed.
The pipeline will immediately stop, and it will start to run the steps detailed in the attribute `error_handling_steps`.

### Creating and running a pipeline

An example of how to create a pipeline can be found in `intercom\pipeline\pipeline.py`.
A `Pipeline` object is created, and we define its attributes `pre_execution_steps`, `pipeline_steps`, `post_execution_steps` and `error_handling_steps` are set.
Note that the attribute `payloads` is not set. It will be determined by `PreExecutionStep` named `GetPayloads`.

An example of running the pipeline can be found in `run_local.py`.
It is run simply by calling the `execute` method.

# The Intercom Conversation Pipeline

The `intercom` module contains the details for running all pipelines related to the Intercom api.
Currently, there is only one model that has a pipeline, which is contained in the `conversation` module.
Future developments are expected to create different pipelines of intercom data, which would be in their own modules.

There is a potential source of problems with querying the intercom api: It is possible for updates to be performed between API calls and therefore an inconsistent dataset will be loaded.
The approach taken is to prioritize eventual correctness:

* If an update is performed while the pipeline is running, it may be excluded from the current pipeline run
* If an update is excluded from the current run, it will be included in the next run (unless it is also being updated during that run)

Details for the conversation model are below.

## The pre-execution steps

There are two pre-execution steps, both of which are in the file `pre_execution_steps.py`.

### GetDatabaseConnection

In the step `GetDatabaseConnection`, we get the database connection and use that connection to create a cursor.
We set this as attributes of the `Pipeline` object, as we will be using them later.

We will want to perform all our database updates in a single transaction, to ensure that we do not miss any updates or have a partially updated database.

### GetPayloads

In the step `GetPayloads`, we generate the list of payloads that we will be processing in this pipeline.
Each payload is a conversation that will be loaded.

In order to prevent redundant work, we only want to load into the database conversations that have been changed since the last time the pipeline was run.
We identify these by using the `updated_at` attribute of the conversation.
We will query the database to find the most recent `updated_at` of the previously loaded conversations and refer to it as `start_timestamp`.
This operation is efficient because we have set up an index on the table `conversations`.

In order to gain stable datasets, we will also need to calculate an `end_timestamp` such that we know that no new updates will be created between the `start_timestamp` and the `end_timestamp`.
We do this by querying the API for conversations that have an `updated_at` greater than `start_timestamp`. We pick the highest value of `updated_at` in the first page of results as our `end_timestamp`.
The documentation does not state what order the results are returned in, but in testing it appeared to be descending order of `updated_at`, guaranteeing that the maximum value is in the first page of results.

Once we have the values of `start_timestamp` and `end_timestamp`, we query the API looking for conversations with an updated_at in the half-open interval `(start_timestamp, end_timestamp]`.
This may be spread out over multiple pages, and so it may require multiple API calls.
It is possible for one of the conversations to be updated between calls, and so we start at the last page and iterate towards the first page.
This guarantees that we will capture every conversation at the risk of capturing one more than once, but capturing one more than once does not put us in danger of inconsistent data sets.

Once we have identified the set of conversations that we want to load, we generate a list of `Payload` objects, where each payload represents a conversation to be loaded.
For each `Payload`, we set its attributes to the values that will be used in the later steps.
The full list of payloads is used to define the attribute `payloads` of the pipeline.

## The pipline steps

There are three pipeline steps, found in the file `pipeline_steps.py`.

### RetrieveApiData

In the step `RetrieveApiData` we make an API call to retrieve the details from a single conversation.
Since all the conversation data is contained within a single API call, we don't need to worry about inconsistent data sets.

Note that the API will not return more than 500 conversation items, therefore if a conversation has over 500 conversation elements, only the most recent 500 will be returned.
If a conversation has over 500 new items since the last time the pipeline was run, then this may result in data being missed.
This is an API limitation.

### ParseAPIData

In the step `ParseApiData` we interpret the API data and generate several dataframes.
A dataframe will be generated for each of the tables that data will be loaded into.

Some dataframes contain more complex objects, which are currently flattened to a single string.
It would be the scope of future developments to expand the loading process for these.

Not all of these api data sections are guaranteed to be populated, so some dataframes may contain zero rows.

### LoadToDatabase

In the step `LoadToDatabase` we load the dataframes into the database.

It is possible for the conversation data to change between pipeline runs.
Since it is possible for data to be present in a previous pipeline run but not the current run, a merge query is insufficient to replace the old data with new data.
Instead, the old tables are deleted from and repopulated for all tables except `conversation_parts`.

For `conversation_parts` there are two things to note - We cannot be certain that the data returned from the API is complete, and we do not expect to ever need to delete old data.
For these reasons we insert use a merge to update any rows in the table that match the new rows by ID and insert new rows if they do not exist in the table.

## The post-execution steps

There is only one post-execution step, in the file `post_execution_steps.py`.
In it, we commit our transaction.
We make sure we don't commit until all the data has been loaded, as otherwise an error in the pipeline can cause us to miss on loading data.

## The error handling steps

There is only one error handling step, in the file `error_handling_steps.py`.
In it, we rollback out transaction.
Rolling back is important to make sure we can correctly determine the value of `start_timestamp` for future pipeline runs.

# Future developments

Some ways to develop this in the future are:

* Add in parallel execution for payloads (or the ability to execute payloads in parallel)
* Add additional intercom ETL pipelines for more data models
* Add in deployment and testing setups
* Add a schedule that the pipeline is run on
  * The original requirements were to run once a day, but it can be run every few minutes with little downside
  * We could potentially miss data if a single conversation has more than 500 parts added between pipeline runs, so the schedule should be frequent enough to prevent 500 parts being added between runs
  * If the pipeline is run too infrequently, the size of the transaction to load all the data may be too large
* The initial pipeline run will attempt to load the full history in a single transaction which may be too large, so it may be necessary to restrict how large a difference there can be between `start_timestamp` and `end_timestamp` 
* Add in singleton logic to prevent the pipeline being run twice in parallel
* Add in notifications for if the pipeline fails
* Add in logic to verify that the api data is structured correctly
* The payloads can be small and there may be a lot of time spent on pipeline overhead which can be reduced by combining multiple conversations into a single payload